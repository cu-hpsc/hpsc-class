{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_c(filename):\n",
    "    from IPython.display import Markdown\n",
    "    with open(filename) as f:\n",
    "        contents = f.read()\n",
    "    return Markdown(\"```c\\n\" + contents + \"```\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processes and Threads\n",
    "\n",
    "Threads and processes are very similar\n",
    "* Both created via [`clone` system call](https://linux.die.net/man/2/clone) on Linux\n",
    "* Scheduled in the same way by the operating system\n",
    "* Separate stacks (automatic variables)\n",
    "* Access to same memory before `fork()` or `clone()`\n",
    "\n",
    "with some important distinctions\n",
    "\n",
    "* Threads set `CLONE_VM`\n",
    "  * threads share the same virtual-to-physical address mapping\n",
    "  * threads can access the same data at the same addresses; private data is private only because other threads don't know its address\n",
    "* Threads set `CLONE_FILES`\n",
    "  * threads share file descriptors\n",
    "* Threads set `CLONE_THREAD`, `CLONE_SIGHAND`\n",
    "  * process id and signal handlers shared\n",
    "\n",
    "#### Myths\n",
    "* Processes can't share memory\n",
    "  * `mmap()`, `shm_open()`, and `MPI_Win_allocate_shared()`\n",
    "* Processes are \"heavy\"\n",
    "  * same data structures and kernel scheduling; no difference in context switching\n",
    "  * data from parent is inherited copy-on-write at very low overhead\n",
    "  * startup costs ~100 microseconds to duplicate page tables\n",
    "  * caches are physically tagged; processes can share L1 cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPI: Message Passing Interface\n",
    "\n",
    "* Just a library: plain C, C++, or Fortran compiler\n",
    "  * Two active open source libraries: [MPICH](https://www.mpich.org/) and [Open MPI](https://www.open-mpi.org/)\n",
    "  * Numerous vendor implementations modify/extend these open source implementations\n",
    "  * MVAPICH is an MPICH-derived open source implementation for InfiniBand and related networks\n",
    "* Bindings from many other languages; [mpi4py](https://mpi4py.readthedocs.io/en/stable/) is popular\n",
    "* Scales to millions of processes across ~100k nodes\n",
    "  * Shared memory systems can be scaled up to [~4000 cores](https://www.uvhpc.com/sgi-uv-3000), but latency and price ($) increase\n",
    "* Standard usage: processes are separate on startup\n",
    "* Timeline\n",
    "  * MPI-1 (1994) point-to-point messaging, collectives\n",
    "  * MPI-2 (1997) parallel IO, dynamic processes, one-sided\n",
    "  * MPI-3 (2012) nonblocking collectives, neighborhood collectives, improved one-sided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```c\n",
       "#include <mpi.h>\n",
       "#include <stdio.h>\n",
       "\n",
       "int main(int argc, char **argv) {\n",
       "  MPI_Init(&argc, &argv);   // Must call before any other MPI functions\n",
       "  int size, rank, sum;\n",
       "  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
       "  MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
       "  MPI_Allreduce(&rank, &sum, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n",
       "  printf(\"I am rank %d of %d: sum=%d\\n\", rank, size, sum);\n",
       "  MPI_Finalize();\n",
       "}\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "render_c('mpi-demo.c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may remind you of the top-level OpenMP strategy\n",
    "```c\n",
    "int main() {\n",
    "    #pragma omp parallel\n",
    "    {\n",
    "        int rank = omp_get_thread_num();\n",
    "        int size = omp_get_num_threads();\n",
    "        // your code\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We use the compiler wrapper `mpicc`, but it just passes some flags to the real compiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -pthread -Wl,-rpath -Wl,/usr/lib/openmpi -Wl,--enable-new-dtags -L/usr/lib/openmpi -lmpi\n"
     ]
    }
   ],
   "source": [
    "! mpicc -show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpicc -Wall    mpi-demo.c   -o mpi-demo\n"
     ]
    }
   ],
   "source": [
    "! make CC=mpicc CFLAGS=-Wall mpi-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We use `mpiexec` to run locally.  Clusters/supercomputers often have different job launching programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am rank 0 of 2: sum=1\n",
      "I am rank 1 of 2: sum=1\n"
     ]
    }
   ],
   "source": [
    "! mpiexec -n 2 ./mpi-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can run more MPI processes than cores (or hardware threads), but you might need to use the `--oversubscribe` option because **oversubscription is usually expensive**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am rank 1 of 6: sum=15\n",
      "I am rank 3 of 6: sum=15\n",
      "I am rank 4 of 6: sum=15\n",
      "I am rank 5 of 6: sum=15\n",
      "I am rank 0 of 6: sum=15\n",
      "I am rank 2 of 6: sum=15\n"
     ]
    }
   ],
   "source": [
    "! mpiexec -n 6 --oversubscribe ./mpi-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can use OpenMP within ranks of MPI (but use `MPI_Init_thread()`)\n",
    "* Everything is private by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advice from Bill Gropp\n",
    "\n",
    ">   You want to think about how you decompose your data structures, how\n",
    "    you think about them globally.  [...]  If you were building a house,\n",
    "    you'd start with a set of blueprints that give you a picture of what\n",
    "    the whole house looks like.  You wouldn't start with a bunch of\n",
    "    tiles and say. \"Well I'll put this tile down on the ground, and\n",
    "    then I'll find a tile to go next to it.\"  But all too many people\n",
    "    try to build their parallel programs by creating the smallest\n",
    "    possible tiles and then trying to have the structure of their code\n",
    "    emerge from the chaos of all these little pieces.  You have to have\n",
    "    an organizing principle if you're going to survive making your code\n",
    "    parallel.\n",
    "    -- https://www.rce-cast.com/Podcast/rce-28-mpich2.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communicators\n",
    "\n",
    "* `MPI_COMM_WORLD` contains all ranks in the `mpiexec`.  Those ranks may be on different nodes, even in different parts of the world.\n",
    "* `MPI_COMM_SELF` contains only one rank\n",
    "* Can create new communicators from existing ones\n",
    "```c\n",
    "int MPI_Comm_dup(MPI_Comm comm, MPI_Comm *newcomm);\n",
    "int MPI_Comm_split(MPI_Comm comm, int color, int key, MPI_Comm *newcomm);\n",
    "int MPI_Comm_create(MPI_Comm comm, MPI_Group group, MPI_Comm *newcomm);\n",
    "```\n",
    "* Can spawn new processes (but not supported on all machines)\n",
    "```c\n",
    "int MPI_Comm_spawn(const char *command, char *argv[], int maxprocs,\n",
    "            MPI_Info info, int root, MPI_Comm comm,\n",
    "            MPI_Comm *intercomm, int array_of_errcodes[]);\n",
    "```\n",
    "* Can attach _attributes_ to communicators (useful for library composition)\n",
    "\n",
    "### Collective operations\n",
    "\n",
    "MPI has a rich set of collective operations scoped by communicator, including the following.\n",
    "\n",
    "```c\n",
    "int MPI_Allreduce(const void *sendbuf, void *recvbuf, int count,\n",
    "        MPI_Datatype datatype, MPI_Op op, MPI_Comm comm);\n",
    "int MPI_Reduce(const void *sendbuf, void *recvbuf, int count,\n",
    "        MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm);\n",
    "int MPI_Scan(const void *sendbuf, void *recvbuf, int count,\n",
    "        MPI_Datatype datatype, MPI_Op op, MPI_Comm comm);\n",
    "int MPI_Gather(const void *sendbuf, int sendcount, MPI_Datatype sendtype,\n",
    "        void *recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm);\n",
    "int MPI_Scatter(const void *sendbuf, int sendcount, MPI_Datatype sendtype,\n",
    "        void *recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm);\n",
    "```\n",
    "\n",
    "* Implementations are optimized by vendors for their custom networks, and can be very fast.\n",
    "\n",
    "![Fischer BGP](https://www.mcs.anl.gov/~fischer/gop/bgp_gop_png.png)\n",
    "\n",
    "Notice how the time is basically independent of number of processes $P$, and only a small multiple of the cost to send a single message. Not all networks are this good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point-to-point messaging\n",
    "\n",
    "In addition to collectives, MPI supports messaging directly between individual ranks.\n",
    "\n",
    "![send-recv](mpi-send-recv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Interfaces can be:\n",
    "  * blocking like `MPI_Send()` and `MPI_Recv()`, or\n",
    "  * \"immediate\" (asynchronous), like `MPI_Isend()` and `MPI_Irecv()`.  The immediate varliants return an `MPI_Request`, which must be waited on to complete the send or receive.\n",
    "* Be careful of deadlock when using blocking interfaces.\n",
    "  * I never use blocking send/recv.\n",
    "  * There are also \"synchronous\" `MPI_Ssend` and \"buffered\" `MPI_Bsend`, and nonblocking variants of these, `MPI_Issend`, etc.\n",
    "    * I never use these either (with one cool exception that we'll talk about).\n",
    "* Point-to-point messaging is like the assembly of parallel computing\n",
    "  * It can be good for building libraries, but it's a headache to use directly for most purposes\n",
    "  * Better to use collectives when possible, or higher level libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighbors\n",
    "\n",
    "A common pattern involves communicating with neighbors, often many times in sequence (such as each iteration or time step).\n",
    "\n",
    "![Neighbor comm](mpi-neighbor-grid.png)\n",
    "\n",
    "This can be achieved with\n",
    "* Point-to-point: `MPI_Isend`, `MPI_Irecv`, `MPI_Waitall`\n",
    "* Persistent: `MPI_Send_init` (once), `MPI_Startall`, `MPI_Waitall`.\n",
    "* Neighborhood collectives (need to create special communicator)\n",
    "* One-sided (need to manage safety yourself)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
